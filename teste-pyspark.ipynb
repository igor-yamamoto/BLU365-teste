{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BLU365 - Teste t√©cnico** ![1](img/√≠ndice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementa√ß√£o em Pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este *notebook* √© referente √† solu√ß√£o do teste proposto pela equipe t√©cnica da BLU365. O teste consiste em analisar uma [base de dados](teste.csv), buscando averiguar quais acordos registrados est√£o ou n√£o est√£o v√°lidos. Os crit√©rios b√°sicos para considerar um acordo como v√°lido s√£o:\n",
    "- O valor total do acordo deve ser igual √† soma de todas as parcelas\n",
    "- O vencimento da primeira parcela deve cair em um dia √∫til\n",
    "\n",
    "Este *notebook* possui a vers√£o da an√°lise feita em `pyspark`. Assim, para que seja poss√≠vel rod√°-lo, se faz necess√°rio configurar o *jupyter* como o driver do pyspark.\n",
    "\n",
    "Al√©m deste pacote, ainda √© usado o pacote `wokalendar`, que analisa quais *timestamps* s√£o referentes aos dias de trabalho.\n",
    "\n",
    "Caso deseje saber mais informa√ß√µes, sinta-se livre [para me contatar](https://github.com/igor-yamamoto).\n",
    "\n",
    "\n",
    "üêâ *Code on!* üêâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importa√ß√£o de bibliotecas e dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, when, col, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+-------------+-------------+------------+--------------+\n",
      "|     documento|                nome|            contrato|ValorContrato|ContratoPlano|ValorParcela|DataVencimento|\n",
      "+--------------+--------------------+--------------------+-------------+-------------+------------+--------------+\n",
      "|   76748291623|        Bryan Barros|CX-28715774669644...|        38610|           10|        3861|    2018-05-21|\n",
      "|   21056684887| Maria Julia da Rosa|CX-61718558756223...|         9464|            4|        2366|    2018-05-08|\n",
      "|   31133201431|        Anthony Melo|CX-16373131100994...|        14285|            5|        2857|    2018-05-05|\n",
      "|94110246446680|Dr. Augusto Silveira|CX-18249643754814...|        37158|           22|        1689|    2018-04-05|\n",
      "|64431473462766|         Maysa Lopes|CX-81229770554257...|        21284|           17|        1252|    2018-03-06|\n",
      "|38126148919828|      Daniela Arag√£o|CX-30455534916519...|        40944|           24|        1706|    2018-03-01|\n",
      "|57595143212710|Ana Carolina Almeida|CX-17507630775591...|       135014|           19|        7106|    2018-07-23|\n",
      "|77420034167536|   Ana Julia Azevedo|CX-17926423044338...|         1012|            4|         253|    2018-04-11|\n",
      "|12629454964414|       Gabriel Porto|CX-15467129984234...|        78256|            8|        9782|    2018-07-19|\n",
      "|63780406371560| Dra. Carolina Alves|CX-22947202467649...|       180113|           23|        7831|    2018-09-17|\n",
      "|   21323098330|        Alexia Rocha|CX-22572611179989...|         6574|            1|        6574|    2018-01-02|\n",
      "|   79059093011|     Eduardo Barbosa|CX-15327011071858...|        84285|            9|        9365|    2018-11-23|\n",
      "|24731771820181|       Kaique Novaes|CX-67682040008045...|        20280|            8|        2535|    2018-01-07|\n",
      "|   97251687631|       Marina Ara√∫jo|CX-54877496276326...|        54712|           14|        3908|    2018-05-06|\n",
      "|   89521154659|        Bruno Castro|CX-33596555009923...|        30488|            4|        7622|    2018-09-14|\n",
      "|   43834568201|  Maria Julia Novaes|CX-31884226184053...|        37950|           10|        3795|    2018-03-11|\n",
      "|   75033218140|        Sophie Rocha|CX-18579256966685...|        58620|           15|        3908|    2018-12-02|\n",
      "|   13166406204| Vinicius Cavalcanti|CX-75445333699670...|         9438|           22|         429|    2018-03-08|\n",
      "|   22327794676|      Henrique Pires|CX-31068677754039...|        27321|           21|        1301|    2018-06-15|\n",
      "|   11035036770|          Sofia Lima|CX-12158162905502...|        39225|            5|        7845|    2018-03-19|\n",
      "+--------------+--------------------+--------------------+-------------+-------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados = spark.read.csv(\"teste.csv\", header=True, inferSchema=False, sep = '|', dateFormat='YYYY-mm-dd')\n",
    "dados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- documento: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- contrato: string (nullable = true)\n",
      " |-- ValorContrato: string (nullable = true)\n",
      " |-- ContratoPlano: string (nullable = true)\n",
      " |-- ValorParcela: string (nullable = true)\n",
      " |-- DataVencimento: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_converter = ['ValorContrato', 'ContratoPlano', 'ValorParcela']\n",
    "\n",
    "for i in para_converter:\n",
    "    dados = dados.withColumn(i,\n",
    "                     dados[i].cast(IntegerType())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertendo o campo de data para o tipo datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados.withColumn('DataVencimento', \n",
    "                 to_date(dados.DataVencimento).cast('timestamp')\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- documento: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- contrato: string (nullable = true)\n",
      " |-- ValorContrato: integer (nullable = true)\n",
      " |-- ContratoPlano: integer (nullable = true)\n",
      " |-- ValorParcela: integer (nullable = true)\n",
      " |-- DataVencimento: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando os valores dos acordos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores dos acordos (valor negociado para quitar as d√≠vidas) devem ser iguais ao valor da soma de todas as parcelas do acordo. Assim, √© poss√≠vel procurar pelos valores de acordo que **n√£o s√£o iguais √† quantidade de parcelas vezes o valor de cada parcela**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.filter(dados.ValorContrato != dados.ValorParcela*dados.ContratoPlano).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo, todos os valores dos acordos s√£o iguais ao valor de cada parcela vezes a quantidade de parcelas a serem pagas. Portanto, **nenhum acordo √© invalidado** por esta condi√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando quais contratos vencem em dias √∫teis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar quais contratos vencem em dias √∫teis, ser√° utilizado o pacote `workalendar`. Assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workalendar.america import Brazil\n",
    "\n",
    "cal = Brazil()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√â poss√≠vel averiguar quais s√£o os feriados registrados no pacote. Para tal, √© poss√≠vel identificar que o intervalo de tempo que diz respeito aos dados no arquivo csv √© referente ao ano de $2018$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2018, 12, 25, 0, 0))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dados.agg({'DataVencimento': 'min'}).collect()[0]['min(DataVencimento)'],\n",
    " dados.agg({'DataVencimento': 'max'}).collect()[0]['max(DataVencimento)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2018, 1, 1), 'New year'),\n",
       " (datetime.date(2018, 4, 1), 'Easter Sunday'),\n",
       " (datetime.date(2018, 4, 21), \"Tiradentes' Day\"),\n",
       " (datetime.date(2018, 5, 1), 'Labour Day'),\n",
       " (datetime.date(2018, 9, 7), 'Independence Day'),\n",
       " (datetime.date(2018, 10, 12), 'Our Lady of Aparecida'),\n",
       " (datetime.date(2018, 11, 2), \"All Souls' Day\"),\n",
       " (datetime.date(2018, 11, 15), 'Republic Day'),\n",
       " (datetime.date(2018, 12, 25), 'Christmas Day')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal.holidays(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al√©m dos feriados registrados acima referente ao ano de 2018, ainda se faz necess√°rio considerar [os feriados nacionais registrados no calend√°rio banc√°rio](https://feriadosbancarios.febraban.org.br/feriados.asp?ano=2018). Dentro deles, **Carnaval** (12/02/2018 e 13/02/2018), **Sexta-feira santa** (30/03/2018) e **Corpus Christi** (31/05/2018). \n",
    "\n",
    "Desta forma, √© criado um novo campo sobre o *dataframe* original chamado `DiaUtil`, que leva o valor booleano `True` se for dia √∫til e `False` caso n√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_holiday = udf(lambda x : (cal.is_working_day(x)))\n",
    "                   \n",
    "dados = dados.withColumn('DiaUtil', udf_holiday('DataVencimento'))\n",
    "\n",
    "dados = dados.withColumn('DiaUtil',\n",
    "                         when(((col('DataVencimento') == '2018-02-12 00:00:00') |\n",
    "                               (col('DataVencimento') == '2018-02-13 00:00:00') |\n",
    "                               (col('DataVencimento') == '2018-05-31 00:00:00') |\n",
    "                               (col('DataVencimento') == '2018-03-30 00:00:00')),\n",
    "                              'false').otherwise(col('DiaUtil')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 458, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'workalendar'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4da16c4ef841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiaUtil\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataVencimento\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiaUtil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 458, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'workalendar'\n"
     ]
    }
   ],
   "source": [
    "dados.filter(dados.DiaUtil == False).select(dados.DataVencimento, dados.DiaUtil).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 458, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'workalendar'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e98bb6722544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiaUtil\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataVencimento\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiaUtil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 589, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 254, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 74, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 458, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'workalendar'\n"
     ]
    }
   ],
   "source": [
    "dados.filter(dados.DiaUtil == True).select(dados.DataVencimento, dados.DiaUtil).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.filter(dados.DiaUtil == False).select(dados.DataVencimento, dados.DiaUtil).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, dentro de todos os dados, $316$ contratos possuem vencimento em dias n√£o √∫teis, **tornando-os inv√°lidos**. Assim, √© poss√≠vel passar os valores booleanos do atributo `DiaUtil` para um novo atributo `Valido`, que possue os valores `True` caso o acordo seja v√°lido e `False` quando for inv√°lido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados.withColumn('Valido', dados.DiaUtil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando os CPFs e CNPJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os documentos de [CPF possuem 11 d√≠gitos](https://pt.wikipedia.org/wiki/Cadastro_de_pessoas_f%C3%ADsicas#N%C3%BAmero_de_inscri%C3%A7%C3%A3o), ao passo que [o CNPJ possui 14](https://pt.wikipedia.org/wiki/Cadastro_Nacional_da_Pessoa_Jur%C3%ADdica). Assim, √© poss√≠vel averiguar o comprimento dos dados fornecidos no campo `documento` quando em formato de *string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = dados.withColumn('doc_tipo', when(F.length(col('documento'))==11,'CPF').otherwise('CNPJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.filter(dados.doc_tipo == 'CPF').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.filter(dados.doc_tipo == 'CNPJ').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com o resultado acima, existem $512$ inst√¢ncias referentes ao registros de CNPJ e $488$ √† CPFs, totalizando $1000$ entradas no total. Para manter registro de qual tipo √© cada documento, √© criado o atributo `doc_tipo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do total de **$1000$ acordos** previamente registrados na base, **$316$ s√£o inv√°lidos**. Assim, a quantidade de **acordos v√°lidos gerados √© de $684$**.\n",
    "A invalidez dos acordos gerados se deve ao vencimento em dias n√£o √∫teis. Ainda, √© poss√≠vel avaliar a distribui√ß√£o dos acordos em rela√ß√£o aos tipos de documentos associados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.toPandas()['Valido'].value_counts().plot.pie(title='Acordos v√°lidos', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acordos_validos = dados.filter(dados.Valido == True)\n",
    "acordos_invalidos = dados.filter(dados.Valido == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(acordos_validos.filter(dados.doc_tipo == 'CPF').count(),\n",
    " acordos_validos.filter(dados.doc_tipo == 'CNPJ').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acordos_validos.toPandas()['doc_tipo'].value_counts().plot.pie(title='Acordos validados', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, dos acordos validados, **$337$ deles s√£o referentes a pessoas f√≠sicas** (CPF, com comprimento de 11) e **$347$ s√£o sobre pessoas jur√≠dicas** (CNPJ, com comprimento de 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(acordos_invalidos.filter(dados.doc_tipo == 'CPF').count(),\n",
    " acordos_invalidos.filter(dados.doc_tipo == 'CNPJ').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acordos_invalidos.toPandas()['doc_tipo'].value_counts().plot.pie(title='Acordos invalidados', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos acordos invalidos, **$151$ s√£o referentes a CPF** e **$165$ s√£o de CNPJ**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor total em reais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os acordos validados, **o montante total gerado √© de R$\\$41.7$ milh√µes**, enquanto os **acordos invalidados geram um total de R$\\$19.1$ milh√µes** (**total: R$\\$60.8$ milh√µes**). Assim, os acordos invalidados representam uma receita de um total de $31.54\\%$, ao passo que para os acordos validados este valor √© de $68.54\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = dados.select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "total_valido = acordos_validos.select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "total_invalido = acordos_invalidos.select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "\n",
    "print('Valor total gerado pelos acordos v√°lidos: R$' +\n",
    "      str(round(total_valido/1e6, 1)) +\n",
    "      ' mi')\n",
    "print('Porcentagem em rela√ß√£o √† todos os acordos (v√°lidos e inv√°lidos): ' +\n",
    "      str(round(100*total_valido/total, 2)) +\n",
    "      '%'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Valor total gerado pelos acordos v√°lidos: R$' +\n",
    "      str(round(total_invalido/1e6, 1)) +\n",
    "      ' mi')\n",
    "print('Porcentagem em rela√ß√£o √† todos os acordos (v√°lidos e inv√°lidos): ' +\n",
    "      str(round(100*total_invalido/total, 2)) +\n",
    "      '%'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos c√°lculos apresentados abaixo, √© poss√≠vel concluir que,\n",
    "- Para os acordos v√°lidos, **R$\\$19.8$ milh√µes derivam de contratos com pessoas f√≠sicas**, enquanto **R$\\$21.9$ milh√µes s√£o relacionados a pessoas jur√≠dicas**\n",
    "- J√° para os acordos inv√°lidos, os contratos com **CPF representam um montante de R$\\$9.1$ milh√µes**, ao passo que, para o caso de **CNPJ, este valor √© de R$\\$10.0$** milh√µes\n",
    "- Tanto para os acordos v√°lidos quanto para os inv√°lidos, a porcentagem de receita gerado por CPF equivale √† aproximadamente $48.0\\%$. Para CNPJ, este valor √© de $52.0\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acordos_validos.filter(dados.doc_tipo == 'CPF').select('ValorContrato').groupBy().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validos_CPF = acordos_validos.filter(dados.doc_tipo == 'CPF').select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "validos_CNPJ = acordos_validos.filter(dados.doc_tipo == 'CNPJ').select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "print('Acordos v√°lidos:')\n",
    "print('Valor total gerado por CPF: R$' +\n",
    "      str(round(validos_CPF/1e6, 1)) +\n",
    "      ' mi (' +\n",
    "      str(round(100*validos_CPF/total_valido, 2)) +\n",
    "      '%)'\n",
    "     )\n",
    "\n",
    "print('Valor total gerado por CPF: R$' +\n",
    "      str(round(validos_CNPJ/1e6, 1)) +\n",
    "      ' mi (' +\n",
    "      str(round(100*validos_CNPJ/total_valido, 2)) +\n",
    "      '%)'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidos_CPF = acordos_invalidos.filter(dados.doc_tipo == 'CPF').select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "invalidos_CNPJ = acordos_invalidos.filter(dados.doc_tipo == 'CNPJ').select('ValorContrato').groupBy().sum().collect()[0][0]\n",
    "\n",
    "print('Acordos inv√°lidos:')\n",
    "print('Valor total gerado por CPF: R$' +\n",
    "      str(round(invalidos_CPF/1e6, 1)) +\n",
    "      ' mi (' +\n",
    "      str(round(100*invalidos_CPF/total_invalido, 2)) +\n",
    "      '%)'\n",
    "     )\n",
    "print('Valor total gerado por CNPJ: R$' +\n",
    "      str(round(invalidos_CNPJ/1e6, 1)) +\n",
    "      ' mi (' +\n",
    "      str(round(100*invalidos_CNPJ/total_invalido, 2)) +\n",
    "      '%)'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, um resumo de toda a receita presente no arquivo csv, bem como as proje√ß√µes desta receita sobre o tipo de documento e a validade do contrato, s√£o apresentadas na tabela abaixo. Cada valor na tabela representa milh√µes de reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Total | CPF | CNPJ |\n",
    "| - | - | - | - |\n",
    "| **Total** | $60.8$ | $28.9$ | $31.9$ |\n",
    "| **V√°lido** | $41.7$ | $19.8$ | $21.9$ |\n",
    "| **Inv√°lido** | $19.1$ | $9.1$ | $10.0$ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
